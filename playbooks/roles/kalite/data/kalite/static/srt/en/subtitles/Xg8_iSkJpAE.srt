1
00:00:00,000 --> 00:00:06,400
In the last couple of videos we first figured out the TOTAL variation in these 9 data points right here

2
00:00:06,400 --> 00:00:11,533
and we got 30, that's our Total Sum of Squares. Then we asked ourselves,

3
00:00:11,533 --> 00:00:19,533
how much of that variation is due to variation WITHIN each of these groups, versus variation BETWEEN the groups themselves?

4
00:00:19,533 --> 00:00:24,933
So, for the variation within the groups we have our Sum of Squares within.

5
00:00:24,933 --> 00:00:26,933
And there we got 6.

6
00:00:26,933 --> 00:00:32,533
And then the balance of this, 30, the balance of this variation,

7
00:00:32,533 --> 00:00:36,333
came from variation between the groups, and we calculated it,

8
00:00:36,333 --> 00:00:39,867
We got 24.

9
00:00:39,867 --> 00:00:43,600
What I want to do in this video, is actually use this type of information,

10
00:00:43,600 --> 00:00:49,267
essentially these statistics we've calculated, to do some inferential statistics,

11
00:00:49,267 --> 00:00:53,200
to come to some time of conclusion, or maybe not to come to some type of conclusion.

12
00:00:53,200 --> 00:00:56,867
What I want to do is to put some context around these groups.

13
00:00:56,867 --> 00:01:00,267
We've been dealing with them abstractly right now, but you can imagine

14
00:01:00,267 --> 00:01:03,133
these are the results of some type of experiment.

15
00:01:03,133 --> 00:01:11,867
Let's say that I gave 3 different types of pills or 3 different types of food to people taking a test.

16
00:01:11,867 --> 00:01:13,933
And these are the scores on the test.

17
00:01:13,933 --> 00:01:25,333
So this is food 1, food 2, and then this over here is food 3.

18
00:01:25,333 --> 00:01:33,400
And I want to figure out if the type of food people take going into the test really affect their scores?

19
00:01:33,400 --> 00:01:40,267
If you look at these means, it looks like they perform best in group 3, than in group 2 or 1.

20
00:01:40,267 --> 00:01:44,733
But is that difference purely random? Random chance?

21
00:01:44,733 --> 00:01:50,600
Or can I be pretty confident that it's due to actual differences

22
00:01:50,600 --> 00:01:56,867
in the population means, of all of the people who would ever take food 3 vs food 2 vs food 1?

23
00:01:56,867 --> 00:02:03,733
So, my question here is, are the means and the true population means the same?

24
00:02:03,733 --> 00:02:10,333
This is a sample mean based on 3 samples. But if I knew the true population means--

25
00:02:10,333 --> 00:02:17,800
So my question is: Is the mean of the population of people taking Food 1 equal to the mean of Food 2?

26
00:02:17,800 --> 00:02:22,000
Obviously I'll never be able to give that food to every human being that could

27
00:02:22,000 --> 00:02:25,667
ever live and then make them all take an exam.

28
00:02:25,667 --> 00:02:30,000
But there is some true mean there, it's just not really measurable.

29
00:02:30,000 --> 00:02:35,667
So my question is "this" equal to "this" equal to the mean 3, the true population of mean 3.

30
00:02:35,667 --> 00:02:38,933
And my question is, are these equal?

31
00:02:38,933 --> 00:02:47,800
Because if they're not equal, that means that the type of food given does have some type of impact

32
00:02:47,800 --> 00:02:50,067
on how people perform on a test.

33
00:02:50,067 --> 00:02:55,000
So let's do a little hypothesis test here. Let's say that my null hypothesis

34
00:02:55,000 --> 00:03:01,267
is that the means are the same. Food doesn't make a difference.

35
00:03:01,267 --> 00:03:07,200
"food doesn't make a difference"

36
00:03:07,200 --> 00:03:17,000
and that my Alternate hypothesis is that it does. "It does."

37
00:03:17,000 --> 00:03:19,000
and the way of thinking about this quantitatively

38
00:03:19,000 --> 00:03:20,933
is that if it doesn't make a difference,

39
00:03:20,933 --> 00:03:24,000
the true population means of the groups will be the same.

40
00:03:24,000 --> 00:03:28,733
The true population mean of the group that took food 1 will be the same

41
00:03:28,733 --> 00:03:35,400
as the group that took food 2, which will be the same as the group that took food 3.

42
00:03:35,400 --> 00:03:40,867
If our alternate hypothesis is correct, then these means will not be all the same.

43
00:03:40,867 --> 00:03:43,067
How can we test this hypothesis?

44
00:03:43,067 --> 00:03:47,200
So we're going to assume the null hypothesis, which is

45
00:03:47,200 --> 00:03:49,800
what we always do when we are hypothesis testing,

46
00:03:49,800 --> 00:03:52,600
we're going to assume our null hypothesis.

47
00:03:52,600 --> 00:03:56,267
And then essentially figure out, what are the chances

48
00:03:56,267 --> 00:03:59,267
of getting a certain statistic this extreme?

49
00:03:59,267 --> 00:04:01,200
And I haven't even defined what that statistic is.

50
00:04:01,200 --> 00:04:05,267
So we're going to define--we're going to assume our null hypothesis,

51
00:04:05,267 --> 00:04:08,667
and then we're going to come up with a statistic called the F statistic.

52
00:04:08,667 --> 00:04:11,933
So our F statistic

53
00:04:11,933 --> 00:04:16,600
which has an F distribution--and we won't go real deep into the details of

54
00:04:16,600 --> 00:04:19,067
the F distribution. But you can already start to think of it

55
00:04:19,067 --> 00:04:23,800
as the ratio of two Chi-squared distributions that may or may not have different degrees of freedom.

56
00:04:23,800 --> 00:04:31,933
Our F statistic is going to be the ratio of our Sum of Squares between the samples--

57
00:04:31,933 --> 00:04:37,067
Sum of Squares between

58
00:04:37,067 --> 00:04:41,733
divided by, our degrees of freedom between

59
00:04:41,733 --> 00:04:46,333
and this is sometimes called the mean squares between, MSB,

60
00:04:46,333 --> 00:04:52,333
that, divided by the Sum of Squares within,

61
00:04:52,333 --> 00:04:56,533
so that's what I had done up here, the SSW in blue,

62
00:04:56,533 --> 00:05:01,133
divided by the SSW

63
00:05:01,133 --> 00:05:07,800
divided by the degrees of freedom of the SSwithin, and that was

64
00:05:07,800 --> 00:05:12,267
m (n-1). Now let's just think about what this is doing right here.

65
00:05:12,267 --> 00:05:18,333
If this number, the numerator, is much larger than the denominator,

66
00:05:18,333 --> 00:05:27,333
then that tells us that the variation in this data is due mostly

67
00:05:27,333 --> 00:05:31,600
to the differences between the actual means

68
00:05:31,600 --> 00:05:35,933
and its due less to the variation within the means.

69
00:05:35,933 --> 00:05:40,867
That's if this numerator is much bigger than this denominator over here.

70
00:05:40,867 --> 00:05:45,733
So that should make us believe that there is a difference

71
00:05:45,733 --> 00:05:47,200
in the true population mean.

72
00:05:47,200 --> 00:05:48,733
So if this number is really big,

73
00:05:48,733 --> 00:05:51,333
it should tell us that there is a lower probability

74
00:05:51,333 --> 00:05:53,600
that our null hypothesis is correct.

75
00:05:53,600 --> 00:05:58,533
If this number is really small and our denominator is larger,

76
00:05:58,533 --> 00:06:02,067
that means that our variation within each sample,

77
00:06:02,067 --> 00:06:05,467
makes up more of the total variation than our variation between

78
00:06:05,467 --> 00:06:07,333
the samples. So that means that our variation

79
00:06:07,333 --> 00:06:12,733
within each of these samples is a bigger percentage of the total variation

80
00:06:12,733 --> 00:06:15,200
versus the variation between the samples.

81
00:06:15,200 --> 00:06:17,800
So that would make us believe that "hey! ya know... any difference

82
00:06:17,800 --> 00:06:21,000
we see between the means is probably just random."

83
00:06:21,000 --> 00:06:24,400
And that would make it a little harder to reject the null.

84
00:06:24,400 --> 00:06:26,867
So let's actually calculate it.

85
00:06:26,867 --> 00:06:34,200
So in this case, our SSbetween, we calculated over here, was 24.

86
00:06:34,200 --> 00:06:37,933
and we had 2 degrees of freedom.

87
00:06:37,933 --> 00:06:49,800
And our SSwithin was 6 and we had how many degrees of freedom?

88
00:06:49,800 --> 00:06:52,667
Also, 6. 6 degrees of freedom.

89
00:06:52,667 --> 00:06:58,600
So this is going to be 24/2 which is 12, divided by 1.

90
00:06:58,600 --> 00:07:05,867
Our F statistic that we've calculated is going to be 12.

91
00:07:05,867 --> 00:07:10,867
F stands for Fischer who is the biologist and statistician who came up with this.

92
00:07:10,867 --> 00:07:15,267
So our F statistic is going to be 12.

93
00:07:15,267 --> 00:07:18,067
We're going to see that this is a pretty high number.

94
00:07:18,067 --> 00:07:19,800
Now, one thing I forgot to mention, with any hypothesis test,

95
00:07:19,800 --> 00:07:22,267
we're going to need some type of significance level.

96
00:07:22,267 --> 00:07:24,733
So let's say the significance level that we care about,

97
00:07:24,733 --> 00:07:28,333
for our hypothesis test, is 10%.

98
00:07:28,333 --> 00:07:32,267
0.10 -- which means

99
00:07:32,267 --> 00:07:36,200
that if we assume the null hypothesis, there is

100
00:07:36,200 --> 00:07:40,067
less than a 10% chance of getting the result we got,

101
00:07:40,067 --> 00:07:41,667
of getting this F statistic,

102
00:07:41,667 --> 00:07:44,800
then we will reject the null hypothesis.

103
00:07:44,800 --> 00:07:48,667
So what we want to do is figure out a critical F statistic value,

104
00:07:48,667 --> 00:07:54,000
that getting that extreme of a value or greater, is 10%

105
00:07:54,000 --> 00:07:57,133
and if this is bigger than our critical F statistic value,

106
00:07:57,133 --> 00:07:59,533
then we're going to reject the null hypothesis,

107
00:07:59,533 --> 00:08:01,400
if it's less, we can't reject the null.

108
00:08:01,400 --> 00:08:06,267
So I'm not going to go into a lot of the guts of the F statistic,

109
00:08:06,267 --> 00:08:09,067
but we can already appreciate that each of these Sum of squares

110
00:08:09,067 --> 00:08:12,533
has a Chi-squared distribution. "This" has a Chi-squared distribution,

111
00:08:12,533 --> 00:08:15,200
and "this" has a different Chi-squared distribution

112
00:08:15,200 --> 00:08:17,533
This is a Chi-squared distribution with 2 degrees of freedom,

113
00:08:17,533 --> 00:08:21,333
this is a Chi-squared distribution with--And we haven't normalized it and all of that--

114
00:08:21,333 --> 00:08:24,067
but roughly a Chi squared distribution with 6 degrees of freedom.

115
00:08:24,067 --> 00:08:29,800
So the F distribution is actually the ratio of two Chi-squared distributions

116
00:08:29,800 --> 00:08:34,933
And I got this--this is a screenshot from a professor's course at UCLA,

117
00:08:34,933 --> 00:08:38,533
I hope they don't mind, I need to find us an F table for us to look into.

118
00:08:38,533 --> 00:08:41,800
But this is what an F distribution looks like.

119
00:08:41,800 --> 00:08:43,267
And obviously it's going to look different

120
00:08:43,267 --> 00:08:46,600
depending on the df of the numerator and the denominator.

121
00:08:46,600 --> 00:08:49,200
There's two df to think about,

122
00:08:49,200 --> 00:08:52,533
the numerator degrees of freedom and the denominator degrees of freedom

123
00:08:52,533 --> 00:08:56,933
With that said, let's calculate the critical F statistic,

124
00:08:56,933 --> 00:09:02,867
for alpha is equal to 0.10,

125
00:09:02,867 --> 00:09:06,533
and you're actually going to see different F tables for each different alpha,

126
00:09:06,533 --> 00:09:11,933
where our numerator df is 2, and our denominator df is 6.

127
00:09:11,933 --> 00:09:17,400
So this table that I got, this whole table is for an alpha of 10%

128
00:09:17,400 --> 00:09:23,733
or 0.10, and our numerator df was 2 and our denominator

129
00:09:23,733 --> 00:09:30,133
was 6. So our critical F value is 3.46.

130
00:09:30,133 --> 00:09:40,000
So our critical F value is 3.46--this value right over here is 3.46

131
00:09:40,000 --> 00:09:43,533
The value that we got based on our data is much larger than this,

132
00:09:43,533 --> 00:09:46,267
WAY above it. It's going to have a very, very small p value.

133
00:09:46,267 --> 00:09:48,000
The probability of getting something this extreme,

134
00:09:48,000 --> 00:09:50,667
just by chance, assuming the null hypothesis,

135
00:09:50,667 --> 00:09:54,733
is very low. It's way bigger than our critical F statistic with

136
00:09:54,733 --> 00:09:56,933
a 10% significance level.

137
00:09:56,933 --> 00:10:01,733
So because of that we can reject the null hypothesis.

138
00:10:01,733 --> 00:10:04,400
Which leads us to believe, "you know what, there probably

139
00:10:04,400 --> 00:10:06,600
IS a difference in the population means."

140
00:10:06,600 --> 00:10:09,600
Which tells us there probably is a difference in performance

141
00:10:09,600 --> 00:10:13,467
on an exam if you give them the different foods.
